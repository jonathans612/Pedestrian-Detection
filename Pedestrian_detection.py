# -*- coding: utf-8 -*-
"""Another copy of detectron2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dGdkUv1-gQ9FoG8CrHenyYYhUXUD26wF
"""

!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu125

!pip install 'git+https://github.com/facebookresearch/detectron2.git'

import torch, detectron2
print("Torch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())

from detectron2.utils.logger import setup_logger
setup_logger()
print("Detectron2 installed successfully!")

from google.colab import drive
drive.mount('/content/drive')

"""# Combined"""

import sys
import os
import cv2
import torch
import numpy as np
import math
import time
from PIL import Image
from torchvision import transforms
from collections import deque
from scipy.spatial.distance import cdist

# --- Detectron2 Imports ---
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import MetadataCatalog
from detectron2.structures import Instances

# ==============================================================================
# 1. CONFIGURATION PARAMETERS
# ==============================================================================

# ---- Input/Output Configuration ----
root = "/content/drive/MyDrive/colab_projects/keypoint_analysis/"
src_path = root + "input.mp4"
out_path = root + "input-processed.mp4"

# ---- Processing Parameters ----
max_frames = None # Set to None to process entire video
process_every_nth_frame = 1
smoothing_window_size = 10 # Frames for speed averaging
forecast_time_s = 1 # Forecast 1 seconds into the future
person_confidence_threshold = 0.7 # Confidence threshold for person detection (0.0 to 1.0)

# --- Check if the file exists on disk ---
if not os.path.isfile(src_path):
    print(f"Error: Input video file not found at the specified path: {src_path}")
    exit()

# ==============================================================================
# 2. SETUP ROAD SEGMENTATION MODEL (BiSeNetV2)
# ==============================================================================
print("Setting up BiSeNetV2 for road segmentation...")

# Make sure we can import from the repo
repo = "/content/BiSeNet"
if not os.path.isdir(repo):
    print("Cloning BiSeNetV2 repository...")
    # This command needs to be executed in a shell context in Colab
    # In a real script, you'd use subprocess.run(['git', 'clone', '...'])
    # For Colab, a direct `!` prefix usually works.
    !git clone https://github.com/CoinCheung/BiSeNet.git
assert os.path.isdir(repo), "BiSeNet repo folder not found."
sys.path.append(repo)

# BiSeNetV2 model class
from lib.models.bisenetv2 import BiSeNetV2

# Build model & load weights
n_classes = 19
weight_path = os.path.join(repo, "res/model_final_v2_city.pth")
assert os.path.isfile(weight_path), f"Missing weights at {weight_path}"

road_segmentation_net = BiSeNetV2(n_classes=n_classes)
sd = torch.load(weight_path, map_location="cpu")
road_segmentation_net.load_state_dict(sd, strict=True)
road_segmentation_net.eval().cuda()
print("BiSeNetV2 model loaded successfully.")

# --- Helper Functions for Road Segmentation ---
to_tensor = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.3257, 0.3690, 0.3223), std=(0.2112, 0.2148, 0.2115))
])

def _pad_to_multiple(img_bgr, multiple=32, mode=cv2.BORDER_REFLECT_101):
    h, w = img_bgr.shape[:2]
    th = int(math.ceil(h / multiple) * multiple)
    tw = int(math.ceil(w / multiple) * multiple)
    pad_bottom = th - h
    pad_right  = tw - w
    if pad_bottom == 0 and pad_right == 0:
        return img_bgr, (0, 0, 0, 0)
    padded = cv2.copyMakeBorder(img_bgr, 0, pad_bottom, 0, pad_right, mode)
    return padded, (0, pad_bottom, 0, pad_right)

def infer_logits(img_bgr, net):
    h0, w0 = img_bgr.shape[:2]
    img_pad, (pt, pb, pl, pr) = _pad_to_multiple(img_bgr, 32)
    hp, wp = img_pad.shape[:2]
    img_rgb = cv2.cvtColor(img_pad, cv2.COLOR_BGR2RGB)
    im = Image.fromarray(img_rgb)
    im_tensor = to_tensor(im).unsqueeze(0).cuda()

    with torch.no_grad():
        out = net(im_tensor)
        logits = out[0] if isinstance(out, (list, tuple)) else out
        logits = torch.nn.functional.interpolate(
            logits, size=(hp, wp), mode="bilinear", align_corners=False
        )
        logits = logits[:, :, :h0, :w0]
    return logits.squeeze(0).cpu()

def road_mask_from_logits(logits, threshold=0.5):
    probs = torch.softmax(logits, dim=0)[0].numpy() # road prob = channel 0
    mask = probs > float(threshold)
    return mask.astype(np.uint8)

def draw_road_outline(img_bgr, mask, color=(0, 0, 255), thickness=2): # BGR for red
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    output_frame = img_bgr.copy()
    cv2.drawContours(output_frame, contours, -1, color, thickness)
    return output_frame

def is_point_in_mask(point_coords, mask_2d):
    x, y = int(point_coords[0]), int(point_coords[1])
    h, w = mask_2d.shape
    if 0 <= x < w and 0 <= y < h:
        return mask_2d[y, x] == 1
    return False

# ==============================================================================
# 3. SETUP KEYPOINT DETECTION MODEL (Detectron2)
# ==============================================================================
print("\nSetting up Detectron2 for keypoint detection...")
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(
    "COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml"
))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(
    "COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml"
)
# Use the confidence threshold variable defined at the top
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = person_confidence_threshold
cfg.MODEL.DEVICE = "cuda"
keypoint_predictor = DefaultPredictor(cfg)
metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])
print("Detectron2 model loaded successfully.")

# ==============================================================================
# 4. COMBINED VIDEO PROCESSING
# ==============================================================================
print("\nStarting combined video processing...")

cap = cv2.VideoCapture(src_path)
if not cap.isOpened():
    print(f"Error: Could not open source video: {src_path}")
    exit()

# ---- Video Properties ----
W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
fourcc = cv2.VideoWriter_fourcc(*"mp4v")
writer = cv2.VideoWriter(out_path, fourcc, fps, (W, H))

# ---- Tracking Data Initialization ----
tracked_people = {}
next_person_id = 0
time_delta = process_every_nth_frame / fps

t0 = time.time()
frame_idx = 0
frames_processed = 0
print(f"Processing every {process_every_nth_frame}th frame, up to {max_frames or 'the end'} frames.")

try:
    while True:
        if max_frames is not None and frame_idx >= max_frames:
            print(f"Reached max frame limit of {max_frames}.")
            break

        ok, frame_bgr = cap.read()
        if not ok:
            break

        frame_idx += 1

        if frame_idx % process_every_nth_frame != 0:
            writer.write(frame_bgr)
            continue

        # --- STEP 1: Apply Road Segmentation Outline ---
        logits = infer_logits(frame_bgr, road_segmentation_net)
        road_mask_2d = road_mask_from_logits(logits, threshold=0.5)
        frame_with_road_outline = draw_road_outline(frame_bgr, road_mask_2d, color=(0, 0, 255), thickness=2)

        # --- STEP 2: Run Keypoint Detection on the new frame ---
        outputs = keypoint_predictor(frame_with_road_outline)
        instances = outputs["instances"].to("cpu")
        frames_processed += 1

        # --- STEP 3: Tracking Logic ---
        current_detections = []
        if instances.has("pred_keypoints") and instances.has("pred_boxes"):
            keypoints = instances.pred_keypoints
            boxes = instances.pred_boxes.tensor.numpy()
            for i, person_kps in enumerate(keypoints):
                left_hip, right_hip = person_kps[11], person_kps[12]
                center_x = (left_hip[0] + right_hip[0]) / 2
                center_y = (left_hip[1] + right_hip[1]) / 2
                current_detections.append({
                    'center': np.array([center_x, center_y]),
                    'keypoints': person_kps,
                    'box': boxes[i]
                })

        unmatched_detections = list(range(len(current_detections)))
        if tracked_people:
            prev_ids = list(tracked_people.keys())
            prev_positions = [p['last_pos'] for p in tracked_people.values()]

            if current_detections and prev_positions:
                current_centers = [d['center'] for d in current_detections]
                distances = cdist(np.array(current_centers), np.array(prev_positions))
                for _ in range(len(current_detections)):
                    if distances.size == 0 or np.min(distances) > 50:
                        break
                    det_idx, prev_idx = np.unravel_index(np.argmin(distances), distances.shape)
                    person_id = prev_ids[prev_idx]
                    detection = current_detections[det_idx]
                    dist_moved = np.linalg.norm(detection['center'] - tracked_people[person_id]['last_pos'])
                    speed_pps = dist_moved / time_delta
                    tracked_people[person_id]['speeds'].append(speed_pps)
                    if dist_moved > 0.1:
                        direction = (detection['center'] - tracked_people[person_id]['last_pos']) / dist_moved
                        tracked_people[person_id]['directions'].append(direction)
                    tracked_people[person_id]['last_pos'] = detection['center']
                    tracked_people[person_id]['keypoints'] = detection['keypoints']
                    tracked_people[person_id]['box'] = detection['box']
                    if det_idx in unmatched_detections:
                        unmatched_detections.remove(det_idx)
                    distances[det_idx, :], distances[:, prev_idx] = np.inf, np.inf

        for det_idx in unmatched_detections:
            detection = current_detections[det_idx]
            tracked_people[next_person_id] = {
                'last_pos': detection['center'],
                'speeds': deque(maxlen=smoothing_window_size),
                'directions': deque(maxlen=smoothing_window_size),
                'keypoints': detection['keypoints'],
                'box': detection['box']
            }
            next_person_id += 1

        # --- STEP 4: Visualization ---
        v = Visualizer(frame_with_road_outline[:, :, ::-1], metadata=metadata, scale=1.0)
        vis_instances = Instances(image_size=(H, W))
        if instances.has("pred_keypoints"):
            vis_instances.pred_keypoints = instances.pred_keypoints
        out = v.draw_instance_predictions(vis_instances)
        final_frame_rgb = out.get_image()

        # ***** FIX WAS HERE *****
        final_frame_bgr = cv2.cvtColor(final_frame_rgb, cv2.COLOR_RGB2BGR)

        for person_id, data in tracked_people.items():
            if 'keypoints' not in data or 'box' not in data: continue

            smoothed_speed = np.mean(data['speeds']) if data['speeds'] else 0

            # --- Draw Speed Text ---
            pos = data['last_pos']
            text = f"{smoothed_speed:.1f} pps"
            cv2.putText(final_frame_bgr, text, (int(pos[0]), int(pos[1] - 25)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

            # --- Check current and forecasted positions ---
            draw_red_box = is_point_in_mask(data['last_pos'], road_mask_2d)
            if not draw_red_box and smoothed_speed > 5 and len(data['directions']) > 0:
                avg_direction = np.mean(data['directions'], axis=0)
                norm = np.linalg.norm(avg_direction)
                if norm > 0:
                    avg_direction /= norm
                    forecasted_pos = data['last_pos'] + avg_direction * smoothed_speed * forecast_time_s
                    if is_point_in_mask(forecasted_pos, road_mask_2d):
                        draw_red_box = True

            # --- Draw Red Bounding Box if condition met ---
            if draw_red_box:
                box = data['box']
                x1, y1, x2, y2 = map(int, box)
                cv2.rectangle(final_frame_bgr, (x1, y1), (x2, y2), (0, 0, 255), 2)

        # --- STEP 5: Write Final Frame ---
        writer.write(final_frame_bgr)

finally:
    cap.release()
    writer.release()
    cv2.destroyAllWindows()
    print(f"\nDone. Processed {frames_processed} frames and wrote to {out_path} in {time.time()-t0:.1f}s")